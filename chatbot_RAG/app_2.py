import os
import time
from datetime import date
from pathlib import Path

import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains.question_answering import load_qa_chain


# =========================
# Thi·∫øt l·∫≠p
# =========================
APP_TITLE = "T∆∞ v·∫•n th·ªß t·ª•c: ƒêƒÉng k√Ω k·∫øt h√¥n (C√¥ng d√¢n Vi·ªát Nam)"
APP_DIR = Path(__file__).resolve().parent
KB_PDF_PATH = APP_DIR / "ChiTietTTHC_1.000894.pdf"
MODEL_NAME = "gemini-2.5-flash-lite"
EMBED_MODEL = "models/gemini-embedding-001"

MIN_SECONDS_BETWEEN_REQUESTS = 2
MAX_REQUESTS_PER_DAY = 30

CHUNK_SIZE = 1600
CHUNK_OVERLAP = 200
TOP_K = 4

MAX_OUTPUT_TOKENS = 512
TEMPERATURE = 0.2


# =========================
# Ch·ªëng spam (test)
# =========================
def allow_request():
    now = time.time()
    today = str(date.today())

    st.session_state.setdefault("last_req", 0.0)
    st.session_state.setdefault("count_today", 0)
    st.session_state.setdefault("day", today)

    if st.session_state["day"] != today:
        st.session_state["day"] = today
        st.session_state["count_today"] = 0

    if now - st.session_state["last_req"] < MIN_SECONDS_BETWEEN_REQUESTS:
        return False, f"B·∫°n ƒëang g·ª≠i qu√° nhanh. ƒê·ª£i {MIN_SECONDS_BETWEEN_REQUESTS} gi√¢y nh√©."
    if st.session_state["count_today"] >= MAX_REQUESTS_PER_DAY:
        return False, f"B·∫°n ƒë√£ ƒë·∫°t gi·ªõi h·∫°n {MAX_REQUESTS_PER_DAY} c√¢u h·ªèi h√¥m nay."

    st.session_state["last_req"] = now
    st.session_state["count_today"] += 1
    return True, ""


# =========================
# KB build/load
# =========================
def extract_text_from_pdf(pdf_path: str) -> str:
    p = Path(pdf_path)
    if not p.exists():
        raise FileNotFoundError(f"Kh√¥ng th·∫•y file KB: {pdf_path}")

    reader = PdfReader(str(p))
    text = "\n".join([(page.extract_text() or "") for page in reader.pages]).strip()
    if not text:
        raise ValueError("Kh√¥ng tr√≠ch xu·∫•t ƒë∆∞·ª£c text t·ª´ PDF.")
    return text


@st.cache_resource(show_spinner=True)
def load_kb_vectorstore(api_key: str):
    raw_text = extract_text_from_pdf(KB_PDF_PATH)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )
    chunks = splitter.split_text(raw_text)

    embeddings = GoogleGenerativeAIEmbeddings(
        model=EMBED_MODEL,
        google_api_key=api_key,
    )
    vs = FAISS.from_texts(chunks, embedding=embeddings)
    return vs


@st.cache_resource(show_spinner=False)
def load_qa_chain_cached(api_key: str):
    prompt_template = """
B·∫°n l√† m·ªôt tr·ª£ l√Ω ·∫£o chuy√™n nghi·ªáp h·ªó tr·ª£ gi·∫£i ƒë√°p c√°c th·ªß t·ª•c h√†nh ch√≠nh v·ªÅ ƒêƒÉng k√Ω k·∫øt h√¥n t·∫°i Vi·ªát Nam.
S·ª≠ d·ª•ng th√¥ng tin c√≥ trong NG·ªÆ C·∫¢NH ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c, th√¢n thi·ªán v√† d·ªÖ hi·ªÉu.
N·∫øu th√¥ng tin kh√¥ng c√≥ trong ng·ªØ c·∫£nh nh∆∞ng V·∫™N li√™n quan ƒëƒÉng k√Ω k·∫øt h√¥n:
- Tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung v·ªÅ lu·∫≠t ph√°p Vi·ªát Nam.

N·∫øu c√¢u h·ªèi KH√îNG li√™n quan ƒëƒÉng k√Ω k·∫øt h√¥n
- H√£y t·ª´ ch·ªëi tr·∫£ l·ªùi m·ªôt c√°ch l·ªãch s·ª±.
- H∆∞·ªõng d·∫´n li√™n h·ªá c∆° quan c√≥ th·∫©m quy·ªÅn ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ th√™m.

QUY T·∫ÆC:
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
- Kh√¥ng tr√≠ch d·∫´n nguy√™n vƒÉn, kh√¥ng sao ch√©p c√¢u ch·ªØ t·ª´ NG·ªÆ C·∫¢NH.
- Kh√¥ng gi·∫£i th√≠ch d√†i d√≤ng, kh√¥ng di·ªÖn gi·∫£i lu·∫≠t.
- Tr·∫£ l·ªùi ƒë√∫ng tr·ªçng t√¢m c√¢u h·ªèi, ∆∞u ti√™n c√¢u tr·∫£ l·ªùi ng·∫Øn.

C√ÅCH TR·∫¢ L·ªúI:
- M·ªói bullet t·ªëi ƒëa 1 c√¢u, d∆∞·ªõi 20 t·ª´.
- Kh√¥ng l·∫∑p l·∫°i √Ω.

NG·ªÆ C·∫¢NH:
{context}

C√ÇU H·ªéI:
{question}

TR·∫¢ L·ªúI:
""".strip()

    llm = ChatGoogleGenerativeAI(
        model=MODEL_NAME,
        google_api_key=api_key,
        temperature=TEMPERATURE,
        max_output_tokens=MAX_OUTPUT_TOKENS,
    )
    prompt = PromptTemplate(template=prompt_template, input_variables=["context", "question"])
    return load_qa_chain(llm=llm, chain_type="stuff", prompt=prompt)


def clear_chat():
    st.session_state.messages = [{"role": "assistant", "content": "B·∫°n mu·ªën h·ªèi g√¨ v·ªÅ th·ªß t·ª•c ƒëƒÉng k√Ω k·∫øt h√¥n?"}]


# =========================
# H·ªèi nhanh
# =========================
def quick_answer(option: str) -> str:
    if option == "C√°ch n·ªôp h·ªì s∆°":
        return (
            "- N·ªôp tr·ª±c ti·∫øp t·∫°i Trung t√¢m/ B·ªô ph·∫≠n m·ªôt c·ª≠a n∆°i c√≥ th·∫©m quy·ªÅn.\n"
            "- N·ªôp tr·ª±c tuy·∫øn tr√™n C·ªïng DVCQG ho·∫∑c C·ªïng DVC c·∫•p t·ªânh (ƒë√≠nh k√®m b·∫£n ch·ª•p/b·∫£n sao ƒëi·ªán t·ª≠ theo quy ƒë·ªãnh).\n"
            "- C√≥ th·ªÉ n·ªôp qua d·ªãch v·ª• b∆∞u ch√≠nh (n·∫øu ƒë·ªãa ph∆∞∆°ng h·ªó tr·ª£).\n"
        )
    if option == "Th·ªùi h·∫°n gi·∫£i quy·∫øt":
        return (
            "- Ngay trong ng√†y ti·∫øp nh·∫≠n h·ªì s∆°.\n"
            "- N·∫øu nh·∫≠n h·ªì s∆° sau 15 gi·ªù m√† ch∆∞a gi·∫£i quy·∫øt ƒë∆∞·ª£c ngay: tr·∫£ k·∫øt qu·∫£ trong ng√†y l√†m vi·ªác ti·∫øp theo.\n"
            "- N·∫øu c·∫ßn x√°c minh ƒëi·ªÅu ki·ªán k·∫øt h√¥n: kh√¥ng qu√° 05 ng√†y l√†m vi·ªác.\n"
        )
    if option == "L·ªá ph√≠":
        return (
            "- Mi·ªÖn l·ªá ph√≠ ƒëƒÉng k√Ω k·∫øt h√¥n.\n"
            "- N·∫øu y√™u c·∫ßu c·∫•p b·∫£n sao Tr√≠ch l·ª•c k·∫øt h√¥n: thu ph√≠ theo quy ƒë·ªãnh hi·ªán h√†nh.\n"
        )
    if option == "ƒêi·ªÅu ki·ªán k·∫øt h√¥n":
        return (
            "- Nam t·ª´ ƒë·ªß 20 tu·ªïi, n·ªØ t·ª´ ƒë·ªß 18 tu·ªïi.\n"
            "- Hai b√™n t·ª± nguy·ªán.\n"
            "- Kh√¥ng m·∫•t nƒÉng l·ª±c h√†nh vi d√¢n s·ª±.\n"
            "- Kh√¥ng thu·ªôc c√°c tr∆∞·ªùng h·ª£p c·∫•m k·∫øt h√¥n; Nh√† n∆∞·ªõc kh√¥ng th·ª´a nh·∫≠n h√¥n nh√¢n gi·ªØa nh·ªØng ng∆∞·ªùi c√πng gi·ªõi t√≠nh.\n"
        )
    return "answer is not available in the context"


# =========================
# Main
# =========================
def main():
    st.set_page_config(page_title=APP_TITLE, page_icon="üìÑ")
    st.title(APP_TITLE)

    load_dotenv()
    api_key = os.getenv("GOOGLE_API_KEY") or st.secrets.get("GOOGLE_API_KEY", None)
    if not api_key:
        st.error("Thi·∫øu GOOGLE_API_KEY (set trong Streamlit Secrets ho·∫∑c .env).")
        st.stop()

    if "messages" not in st.session_state:
        clear_chat()

    # Style cho m·ª•c h·ªèi nhanh 
    st.markdown(
        """
        <style>
        .quick-box button {
            width: 100%;
            border-radius: 14px !important;
            border: 1px solid #ddd !important;
            padding: 0.65rem 0.9rem !important;
            margin-bottom: 0.55rem !important;
            background: #f9fafb !important;
            text-align: left !important;
            font-weight: 600 !important;
        }
        .quick-box button:hover {
            border-color: #4f46e5 !important;
            background: #eef2ff !important;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    with st.sidebar:
        st.subheader("H·ªèi nhanh")

        quick_items = [
            "C√°ch n·ªôp h·ªì s∆°",
            "Th·ªùi h·∫°n gi·∫£i quy·∫øt",
            "L·ªá ph√≠",
            "ƒêi·ªÅu ki·ªán k·∫øt h√¥n",
        ]

        # M·ªói m·ª•c l√† 1 box 
        # Click l√† tr·∫£ l·ªùi ngay
        # st.markdown('<div class="quick-box">', unsafe_allow_html=True)
        st.markdown("""
        <style>
        div.stButton > button:first-child {
            width: 100%;
            border-radius: 8px;
            margin-bottom: 5px;
        }
        </style>""", unsafe_allow_html=True)
        
        for item in quick_items:
            if st.button(item, key=f"quick_{item}", use_container_width=True):
                st.session_state.messages.append({"role": "user", "content": item})
                st.session_state.messages.append({"role": "assistant", "content": quick_answer(item)})
        st.markdown("</div>", unsafe_allow_html=True)

        st.divider()
        # st.caption("Ch·ªëng spam (theo session):")
        # st.caption(f"- {MAX_REQUESTS_PER_DAY} c√¢u/ng√†y")
        # st.caption(f"- t·ªëi thi·ªÉu {MIN_SECONDS_BETWEEN_REQUESTS}s/c√¢u")
        st.button("X√≥a l·ªãch s·ª≠ chat", on_click=clear_chat)

    # l·ªãch s·ª≠
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.write(m["content"])

    # Load KB 
    vs = load_kb_vectorstore(api_key)
    chain = load_qa_chain_cached(api_key)

    # input
    if question := st.chat_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n t·∫°i ƒë√¢y..."):
        ok, msg = allow_request()
        if not ok:
            st.warning(msg)
            st.stop()

        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.write(question)

        with st.chat_message("assistant"):
            with st.spinner("ƒêang tra c·ª©u th·ªß t·ª•c..."):
                docs = vs.similarity_search(question, k=TOP_K)
                out = chain({"input_documents": docs, "question": question}, return_only_outputs=True)
                answer = (out or {}).get("output_text", "answer is not available in the context")
                st.write(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})


if __name__ == "__main__":
    main()
